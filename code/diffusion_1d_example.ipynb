{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e79a185",
   "metadata": {},
   "source": [
    "### Simple 1D example of `DDPM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b1e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from util import (\n",
    "    gp_sampler,\n",
    "    get_torch_size_string,\n",
    "    plot_1xN_torch_traj_tensor,\n",
    ")\n",
    "from diffusion import (\n",
    "    get_ddpm_constants,\n",
    "    plot_ddpm_constants,\n",
    "    DiffusionUNet,\n",
    "    forward_sample,\n",
    ")\n",
    "from dataset import mnist\n",
    "np.set_printoptions(precision=3)\n",
    "th.set_printoptions(precision=3)\n",
    "plt.rc('xtick',labelsize=8)\n",
    "plt.rc('ytick',labelsize=8)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "print (\"PyTorch version:[%s].\"%(th.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3919499",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c833c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dc = get_ddpm_constants(\n",
    "    schedule_name = 'cosine', # 'linear', 'cosine'\n",
    "    T             = 1000,\n",
    "    np_type       = np.float32,\n",
    ")\n",
    "for k_idx,key in enumerate(dc.keys()):\n",
    "    print (\"[%2d] key:[%s]\"%(k_idx,key))\n",
    "plot_ddpm_constants(dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d09b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate U-net\n",
    "device = 'cpu'\n",
    "model = DiffusionUNet(\n",
    "    name             = 'unet',\n",
    "    dims             = 1,\n",
    "    n_in_channels    = 1,\n",
    "    n_model_channels = 256,\n",
    "    n_emb_dim        = 256,\n",
    "    n_enc_blocks     = 2, # number of encoder blocks\n",
    "    n_dec_blocks     = 2, # number of decoder blocks\n",
    "    n_groups         = 4, # group norm paramter\n",
    "    device           = device,\n",
    ") # input:[B x C x L] => output:[B x C x L]\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f83e2",
   "metadata": {},
   "source": [
    "### Training data `x_0`: [N x C x L]\n",
    "where N is the number of data, C is the channel size, and L is the length of trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed815f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "n_traj = 3\n",
    "times = np.linspace(start=0.0,stop=1.0,num=100).reshape((-1,1)) # [L x 1]\n",
    "traj = th.from_numpy(\n",
    "    gp_sampler(\n",
    "        times    = times,\n",
    "        hyp_gain = 2.0,\n",
    "        hyp_len  = 0.2,\n",
    "        meas_std = 1e-8,\n",
    "        n_traj   = n_traj\n",
    "    )\n",
    ").to(th.float32).to(device) # [L x 10]\n",
    "# Plot\n",
    "plt.figure(figsize=(6,2))\n",
    "for t_idx in range(n_traj):\n",
    "    plt.plot(times,traj[t_idx,:].cpu().numpy(),ls='-',color='k',lw=1)\n",
    "plt.xlim([0.0,1.0]); plt.ylim([-3,+3]);\n",
    "plt.xlabel('Time',fontsize=10)\n",
    "plt.show()\n",
    "# Print\n",
    "x_0 = traj[:,None,:] # [N x C x L]\n",
    "print (\"x_0:[%s]\"%(get_torch_size_string(x_0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226866b8",
   "metadata": {},
   "source": [
    "### Evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_ddpm_1d(model,dc,n_sample,x_0,device):\n",
    "    \"\"\"\n",
    "        Evaluate DDPM \n",
    "    \"\"\"\n",
    "    n_data,C,L = x_0.shape\n",
    "    x_dummy    = th.zeros(n_sample,C,L,device=device)\n",
    "    step_dummy = th.zeros(n_sample).type(th.long).to(device)\n",
    "    _,x_T      = forward_sample(x_dummy,step_dummy,dc) # [n_sample x C x L]\n",
    "    x_t        = x_T.clone() # [n_sample x C x L]\n",
    "    x_t_list   = ['']*dc['T']\n",
    "    for t in range(0,dc['T'])[::-1]: # 999 to 0\n",
    "        # Score function\n",
    "        step = th.full(\n",
    "            size       = (n_sample,),\n",
    "            fill_value = t,\n",
    "            device     = device,\n",
    "            dtype      = th.long) # [n_sample]\n",
    "        eps_t,_ = model(x_t,step) # [n_sample x C x L]\n",
    "        betas_t = th.gather(\n",
    "            input = th.from_numpy(dc['betas']).to(device), # [T]\n",
    "            dim   = -1,\n",
    "            index = step\n",
    "        ).reshape((-1,1,1)) # [n_sample x 1 x 1]\n",
    "        sqrt_one_minus_alphas_bar_t = th.gather(\n",
    "            input = th.from_numpy(dc['sqrt_one_minus_alphas_bar']).to(device), # [T]\n",
    "            dim   = -1,\n",
    "            index = step\n",
    "        ).reshape((-1,1,1)) # [n_sample x 1 x 1]\n",
    "        sqrt_recip_alphas_t = th.gather(\n",
    "            input = th.from_numpy(dc['sqrt_recip_alphas']).to(device), # [T]\n",
    "            dim   = -1,\n",
    "            index = step\n",
    "        ).reshape((-1,1,1)) # [n_sample x 1 x 1]\n",
    "        # Compute posterior mean\n",
    "        mean_t = sqrt_recip_alphas_t * (\n",
    "            x_t - betas_t*eps_t/sqrt_one_minus_alphas_bar_t\n",
    "            ) # [n_sample x C x L]\n",
    "        # Compute posterior variance\n",
    "        posterior_variance_t = th.gather(\n",
    "            input = th.from_numpy(dc['posterior_variance']).to(device), # [T]\n",
    "            dim   = -1,\n",
    "            index = step\n",
    "        ).reshape((-1,1,1)) # [n_sample x 1 x 1]\n",
    "        # Sample\n",
    "        if t == 0: # last sampling, use mean\n",
    "            x_t = mean_t\n",
    "        else:\n",
    "            _,noise_t = forward_sample(x_dummy,step_dummy,dc) # [n_sample x C x 1]\n",
    "            x_t = mean_t + th.sqrt(posterior_variance_t)*noise_t\n",
    "        # Append\n",
    "        x_t_list[t] = x_t\n",
    "    \n",
    "    # Plot\n",
    "    for t in np.linspace(dc['T']-1,0,5).astype(np.int32):\n",
    "        x_t = x_t_list[t] # [n_sample x C x L]\n",
    "        x_t_np = x_t.detach().cpu().numpy() # [n_sample x C x L]\n",
    "        x_0_np = x_0.detach().cpu().numpy() # [n_data x C x L]\n",
    "        plt.figure(figsize=(6,2))\n",
    "        for i_idx in range(n_data):\n",
    "            plt.plot(times.flatten(),x_0_np[i_idx,0,:],ls='-',color='b',lw=1)\n",
    "        for i_idx in range(n_sample):\n",
    "            plt.plot(times.flatten(),x_t_np[i_idx,0,:],ls='-',color='k',lw=1)\n",
    "        plt.xlim([0.0,1.0])\n",
    "        # plt.ylim([-3,+3])\n",
    "        plt.xlabel('Time',fontsize=8)\n",
    "        plt.title('Step:[%d]'%(t),fontsize=8)\n",
    "        plt.show()\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f9c6b5",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba18376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iter    = int(1e5)\n",
    "batch_size  = 128\n",
    "print_every = 1e3\n",
    "eval_every  = 1e4\n",
    "\n",
    "# Loop\n",
    "model.train()\n",
    "optm = th.optim.AdamW(params=model.parameters(),lr=1e-4,weight_decay=1e-8)\n",
    "schd = th.optim.lr_scheduler.ExponentialLR(optimizer=optm,gamma=0.998)\n",
    "for it in range(max_iter):\n",
    "    \n",
    "    # Zero gradient\n",
    "    optm.zero_grad()\n",
    "    \n",
    "    # Get batch\n",
    "    idx = np.random.choice(x_0.shape[0],batch_size)\n",
    "    x_0_batch = x_0[idx,:,:] # [B x C x L]\n",
    "    \n",
    "    # Sample time steps\n",
    "    step_batch = th.randint(0, dc['T'],(batch_size,),device=device).long() # [B]\n",
    "    \n",
    "    # Forward diffusion sampling\n",
    "    x_t_batch,noise = forward_sample(x_0_batch,step_batch,dc) # [B x C x L]\n",
    "\n",
    "    # Model predict\n",
    "    noise_pred,_ = model(x_t_batch,step_batch) # [B x C x L]\n",
    "    \n",
    "    # Compute error\n",
    "    loss = F.mse_loss(noise,noise_pred)\n",
    "    \n",
    "    # Update\n",
    "    loss.backward()\n",
    "    optm.step()\n",
    "    schd.step()\n",
    "    \n",
    "    # Print\n",
    "    if (it%print_every) == 0 or it == (max_iter-1):\n",
    "        print (\"it:[%7d][%.1f]%% loss:[%.4f]\"%(it,100*it/max_iter,loss.item()))\n",
    "    \n",
    "    # Evaluate\n",
    "    if (it%eval_every) == 0 or it == (max_iter-1):\n",
    "        n_sample = 10\n",
    "        eval_ddpm_1d(model,dc,n_sample,x_0,device)\n",
    "    \n",
    "print (\"Done.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75576a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb68d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
