{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71051e5d",
   "metadata": {},
   "source": [
    "### Attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae3d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[2.0.1].\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from util import get_torch_size_string\n",
    "np.set_printoptions(precision=3)\n",
    "th.set_printoptions(precision=3)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "print (\"PyTorch version:[%s].\"%(th.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ba648",
   "metadata": {},
   "source": [
    "### QKV Attention (Legacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6dec573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "class GroupNorm32(nn.GroupNorm):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "    \n",
    "def normalization(n_channels,n_groups=1):\n",
    "    \"\"\"\n",
    "    Make a standard normalization layer.\n",
    "\n",
    "    :param n_channels: number of input channels.\n",
    "    :param n_channels: number of input channels. if this is 1, then it is identical to layernorm.\n",
    "    :return: an nn.Module for normalization.\n",
    "    \"\"\"\n",
    "    return GroupNorm32(num_groups=n_groups,num_channels=n_channels)\n",
    "\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb29398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "class QKVAttentionLegacy(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which performs QKV attention. Matches legacy QKVAttention + input/ouput heads shaping\n",
    "    \"\"\"\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = th.einsum(\n",
    "            \"bct,bcs->bts\", q * scale, k * scale\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = th.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)\n",
    "    \n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention block that allows spatial positions to attend to each other.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_channels         = 1,\n",
    "            n_heads            = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_channels         = n_channels\n",
    "        self.n_heads            = n_heads\n",
    "        assert (\n",
    "            n_channels % n_heads == 0\n",
    "        ), f\"n_channels:[%d] should be divisible by n_heads:[%d].\"%(n_channels,n_heads)\n",
    "            \n",
    "        # Normalize \n",
    "        self.norm = normalization(n_channels=n_channels,n_groups=32)\n",
    "        \n",
    "        # Tripple the channel\n",
    "        self.qkv = nn.Conv1d(\n",
    "            in_channels  = self.n_channels,\n",
    "            out_channels = self.n_channels*3,\n",
    "            kernel_size  = 1\n",
    "        )\n",
    "        \n",
    "        # QKV Attention\n",
    "        self.attention = QKVAttentionLegacy(\n",
    "            n_heads = self.n_heads\n",
    "        )\n",
    "        \n",
    "        # Projection\n",
    "        self.proj_out = zero_module(\n",
    "            nn.Conv1d(\n",
    "                in_channels  = self.n_channels,\n",
    "                out_channels = self.n_channels,\n",
    "                kernel_size  = 1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [B x C x W x H] tensor\n",
    "        :return out: [B x C x W x H] tensor\n",
    "        \"\"\"\n",
    "        b, c, *spatial = x.shape\n",
    "        # Triple the channel \n",
    "        x   = x.reshape(b, c, -1)    # [B x C x WH]\n",
    "        x   = self.norm(x)           # [B x C x WH]\n",
    "        qkv = self.qkv(x)            # [B x 3C x WH]\n",
    "        # QKV attention\n",
    "        h   = self.attention(qkv)    # [B x C x WH]\n",
    "        h   = self.proj_out(h)       # [B x C x WH]\n",
    "        out = (x + h).reshape(b, c, *spatial) # [B x C x WH]\n",
    "        return out\n",
    "\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7e801",
   "metadata": {},
   "source": [
    "### Let's see how `AttentionBlock` works\n",
    "- First, we assume that an input tensor has a shape of [B x C x W x H].\n",
    "- This can be thought of having a total of WH tokens with each token hainv C dimensions. \n",
    "- The MHA operates by initally partiting the channels, executing qkv attention process, and then merging the results. \n",
    "- Note the the number of channels should be divisible by the number of heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c6e06cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:[16x128x28x28] output shape:[16x128x28x28]\n"
     ]
    }
   ],
   "source": [
    "layer = AttentionBlock(n_channels=128,n_heads=4)\n",
    "x = th.randn(16,128,28,28)\n",
    "out = layer(x)\n",
    "print (\"input shape:[%s] output shape:[%s]\"%\n",
    "       (get_torch_size_string(x),get_torch_size_string(out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902acd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
