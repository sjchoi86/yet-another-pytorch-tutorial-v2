{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71051e5d",
   "metadata": {},
   "source": [
    "### Attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae3d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[2.0.1].\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "from module import AttentionBlock\n",
    "from util import get_torch_size_string\n",
    "np.set_printoptions(precision=3)\n",
    "th.set_printoptions(precision=3)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "print (\"PyTorch version:[%s].\"%(th.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7e801",
   "metadata": {},
   "source": [
    "### Let's see how `AttentionBlock` works\n",
    "- First, we assume that an input tensor has a shape of [B x C x W x H].\n",
    "- This can be thought of having a total of WH tokens with each token having C dimensions. \n",
    "- The MHA operates by initally partiting the channels, executing qkv attention process, and then merging the results. \n",
    "- Note the the number of channels should be divisible by the number of heads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0e4fa",
   "metadata": {},
   "source": [
    "### `dims=2`\n",
    "#### `x` has a shape of `[B x C x W x H]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6e06cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:[16x128x28x28] output shape:[16x128x28x28]\n",
      "[         x]:[   16x128x28x28]\n",
      "[     x_rsh]:[     16x128x784]\n",
      "[     x_nzd]:[     16x128x784]\n",
      "[       qkv]:[     16x384x784]\n",
      "[     h_att]:[     16x128x784]\n",
      "[    h_proj]:[     16x128x784]\n",
      "[       out]:[   16x128x28x28]\n"
     ]
    }
   ],
   "source": [
    "layer = AttentionBlock(n_channels=128,n_heads=4,n_groups=32)\n",
    "x = th.randn(16,128,28,28)\n",
    "out,intermediate_output_dict = layer(x)\n",
    "print (\"input shape:[%s] output shape:[%s]\"%\n",
    "       (get_torch_size_string(x),get_torch_size_string(out)))\n",
    "# Print intermediate values\n",
    "for key,value in intermediate_output_dict.items():\n",
    "    print (\"[%10s]:[%15s]\"%(key,get_torch_size_string(value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d9cec",
   "metadata": {},
   "source": [
    "### `dims=1`\n",
    "#### `x` has a shape of `[B x C x L]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d1bbdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:[16x4x100] output shape:[16x4x100]\n",
      "[         x]:[       16x4x100]\n",
      "[     x_rsh]:[       16x4x100]\n",
      "[     x_nzd]:[       16x4x100]\n",
      "[       qkv]:[      16x12x100]\n",
      "[     h_att]:[       16x4x100]\n",
      "[    h_proj]:[       16x4x100]\n",
      "[       out]:[       16x4x100]\n"
     ]
    }
   ],
   "source": [
    "layer = AttentionBlock(n_channels=4,n_heads=2,n_groups=1)\n",
    "x = th.randn(16,4,100)\n",
    "out,intermediate_output_dict = layer(x)\n",
    "print (\"input shape:[%s] output shape:[%s]\"%\n",
    "       (get_torch_size_string(x),get_torch_size_string(out)))\n",
    "# Print intermediate values\n",
    "for key,value in intermediate_output_dict.items():\n",
    "    print (\"[%10s]:[%15s]\"%(key,get_torch_size_string(value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60d073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
